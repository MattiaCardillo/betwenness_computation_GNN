{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T12:07:50.561419Z","iopub.status.busy":"2024-09-10T12:07:50.560572Z","iopub.status.idle":"2024-09-10T12:08:18.696553Z","shell.execute_reply":"2024-09-10T12:08:18.695572Z","shell.execute_reply.started":"2024-09-10T12:07:50.561380Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting torch_geometric\n","  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (4.66.4)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.26.4)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.14.0)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2024.6.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.4)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.9.5)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2.32.3)\n","Requirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.2)\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (5.9.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch_geometric) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (2024.7.4)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (3.5.0)\n","Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: torch_geometric\n","Successfully installed torch_geometric-2.5.3\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (3.3)\n"]}],"source":["#!pip install torch_geometric\n","#!pip install networkx"]},{"cell_type":"markdown","metadata":{},"source":["# Obiettivi\n","Creare una rete neurale capace di calcolare la **betweenness centrality** di un nodo all'interno di un grafo con una precisione accettabile, comparabile agli algoritmi di approssimazione tradizionali, ma con una significativa riduzione dei tempi di calcolo, quindi della complessità computazionale.\n","\n","## Attività principali:\n","1. **Selezione del dataset (grafo) di training**\n","2. **Calcolo della betweenness centrality esatta** per il grafo di training\n","3. **Data labelling**: Aggiunta della feature *betweenness centrality* al grafo\n","4. **Suddivisione Dataset in test e val**\n","5. parallelismo -> immagine/grafo nodo/pixel\n","5. **Sviluppo del modello**:\n","    - Modello di regressione\n","    - Training supervisionato\n","6. **Training del modello**: con test e validazione\n","7. **Test delle prestazioni del modello**\n","8. **Confronto** dei risultati ottenuti con gli approcci classici\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1. **Selezione del dataset (grafo) di training**\n","\n","The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch_geometric\n","import networkx as nx\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.utils import to_networkx"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Utilizza tutti i dataset disponibili su Planetoid\n","datasets = ['Cora', 'CiteSeer', 'PubMed']\n","data_list = []\n","\n","for dataset_name in datasets:\n","    dataset = Planetoid(root=f'/tmp/{dataset_name}', name=dataset_name)\n","    data_list.append(dataset[0])"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T12:08:18.699908Z","iopub.status.busy":"2024-09-10T12:08:18.698975Z","iopub.status.idle":"2024-09-10T12:08:25.543833Z","shell.execute_reply":"2024-09-10T12:08:25.542833Z","shell.execute_reply.started":"2024-09-10T12:08:18.699856Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Numero di nodi: 2708\n","Train mask: 140 nodi\n","Validation mask: 500 nodi\n","Test mask: 1000 nodi\n","Numero di nodi: 3327\n","Train mask: 120 nodi\n","Validation mask: 500 nodi\n","Test mask: 1000 nodi\n","Numero di nodi: 19717\n","Train mask: 60 nodi\n","Validation mask: 500 nodi\n","Test mask: 1000 nodi\n"]}],"source":["# Stampa il tipo di ogni dataset caricato\n","for data in data_list:\n","    print(f'Numero di nodi: {data.num_nodes}')\n","    print(f'Train mask: {data.train_mask.sum()} nodi')\n","    print(f'Validation mask: {data.val_mask.sum()} nodi')\n","    print(f'Test mask: {data.test_mask.sum()} nodi')"]},{"cell_type":"markdown","metadata":{},"source":["### 2. Calcolo della betweenness centrality esatta per il grafo di training\n","### 3. Data Labelling\n"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-10T07:00:47.027666Z","iopub.status.busy":"2024-09-10T07:00:47.027097Z","iopub.status.idle":"2024-09-10T07:00:47.031767Z","shell.execute_reply":"2024-09-10T07:00:47.030686Z","shell.execute_reply.started":"2024-09-10T07:00:47.027622Z"},"trusted":true},"outputs":[],"source":["for data in data_list:\n","    G = to_networkx(data, to_undirected=True)\n","\n","    # Calcola la betweenness centrality dei nodi\n","    betweenness = nx.betweenness_centrality(G)\n","\n","    data.y = torch.tensor([betweenness[i] for i in range(data.num_nodes)], dtype=torch.float)"]},{"cell_type":"markdown","metadata":{},"source":["#### Save betweenness computation offline"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import pickle\n","dataset_path = \"/tmp/dataset_with_betweenness.pkl\""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["with open(dataset_path, 'wb') as f:\n","    pickle.dump(data_list, f)"]},{"cell_type":"markdown","metadata":{},"source":["**import from offline**"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T07:00:47.047587Z","iopub.status.busy":"2024-09-10T07:00:47.047050Z","iopub.status.idle":"2024-09-10T07:00:47.529368Z","shell.execute_reply":"2024-09-10T07:00:47.528462Z","shell.execute_reply.started":"2024-09-10T07:00:47.047516Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708]), Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327]), Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717])]\n"]}],"source":["with open(dataset_path, 'rb') as f:\n","    data_list = pickle.load(f)\n","\n","print(data_list)"]},{"cell_type":"markdown","metadata":{},"source":["### 4. Suddivisione Dataset in test e val\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T07:00:47.530966Z","iopub.status.busy":"2024-09-10T07:00:47.530628Z","iopub.status.idle":"2024-09-10T07:00:47.534810Z","shell.execute_reply":"2024-09-10T07:00:47.533905Z","shell.execute_reply.started":"2024-09-10T07:00:47.530932Z"},"trusted":true},"outputs":[],"source":["# nulla da fare, i datasets sono già suddivisi."]},{"cell_type":"markdown","metadata":{},"source":["### 5. Sviluppo del modello\n","\n","**guardare nn_model.py**"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["from nn_model import *\n","import torch\n"]},{"cell_type":"markdown","metadata":{},"source":["### 6. Training\n","#### Model setup"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["def add_padding_to_features(data, target_num_features):\n","    num_features = data.x.shape[1]\n","    if num_features < target_num_features:\n","        padding = torch.zeros((data.num_nodes, target_num_features - num_features), dtype=torch.float)\n","        data.x = torch.cat([data.x, padding], dim=1)\n","    return data"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["from torch_geometric.loader import DataLoader\n","\n","# Definizione dei parametri del modello\n","target_num_features = max(data.num_node_features for data in data_list)\n","hidden_channels = 64\n","out_channels = 1\n","\n","data_list = [add_padding_to_features(data, target_num_features) for data in data_list]\n","\n","train_loader = DataLoader([data for data in data_list if data.train_mask.sum() > 0], batch_size=32, shuffle=True)\n","val_loader = DataLoader([data for data in data_list if data.val_mask.sum() > 0], batch_size=32, shuffle=False)\n","test_loader = DataLoader([data for data in data_list if data.test_mask.sum() > 0], batch_size=32, shuffle=False)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T07:00:47.573600Z","iopub.status.busy":"2024-09-10T07:00:47.572982Z","iopub.status.idle":"2024-09-10T07:00:47.970867Z","shell.execute_reply":"2024-09-10T07:00:47.970080Z","shell.execute_reply.started":"2024-09-10T07:00:47.573568Z"},"trusted":true},"outputs":[],"source":["model = GATRegression(target_num_features, hidden_channels, out_channels).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Training functions"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T07:00:47.972187Z","iopub.status.busy":"2024-09-10T07:00:47.971916Z","iopub.status.idle":"2024-09-10T07:00:47.978567Z","shell.execute_reply":"2024-09-10T07:00:47.977748Z","shell.execute_reply.started":"2024-09-10T07:00:47.972157Z"},"trusted":true},"outputs":[],"source":["def train(loader):\n","    model.train()\n","    total_loss = 0\n","    for data in loader:\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","        out = model(data)\n","        loss = F.mse_loss(out[data.train_mask], data.y[data.train_mask].unsqueeze(1))\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(loader)\n","\n","def test(loader):\n","    model.eval()\n","    total_loss = 0\n","    with torch.no_grad():\n","        for data in loader:\n","            data = data.to(device)\n","            out = model(data)\n","            loss = F.mse_loss(out[data.test_mask], data.y[data.test_mask].unsqueeze(1))\n","            total_loss += loss.item()\n","    return total_loss / len(loader)"]},{"cell_type":"markdown","metadata":{},"source":["#### Training Phase"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T07:00:47.979991Z","iopub.status.busy":"2024-09-10T07:00:47.979700Z","iopub.status.idle":"2024-09-10T07:00:47.990341Z","shell.execute_reply":"2024-09-10T07:00:47.989640Z","shell.execute_reply.started":"2024-09-10T07:00:47.979960Z"},"trusted":true},"outputs":[],"source":["num_epochs = 300"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T07:00:47.991735Z","iopub.status.busy":"2024-09-10T07:00:47.991388Z","iopub.status.idle":"2024-09-10T07:06:38.076881Z","shell.execute_reply":"2024-09-10T07:06:38.075803Z","shell.execute_reply.started":"2024-09-10T07:00:47.991693Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 001, Train Loss: 0.0825, Val Loss: 0.0073, Test Loss: 0.0073\n","Epoch: 002, Train Loss: 0.0887, Val Loss: 0.0061, Test Loss: 0.0061\n","Epoch: 003, Train Loss: 0.0748, Val Loss: 0.0049, Test Loss: 0.0049\n","Epoch: 004, Train Loss: 0.0828, Val Loss: 0.0047, Test Loss: 0.0047\n","Epoch: 005, Train Loss: 0.0710, Val Loss: 0.0046, Test Loss: 0.0046\n","Epoch: 006, Train Loss: 0.0635, Val Loss: 0.0042, Test Loss: 0.0042\n","Epoch: 007, Train Loss: 0.0490, Val Loss: 0.0041, Test Loss: 0.0041\n","Epoch: 008, Train Loss: 0.0767, Val Loss: 0.0043, Test Loss: 0.0043\n","Epoch: 009, Train Loss: 0.0683, Val Loss: 0.0050, Test Loss: 0.0050\n","Epoch: 010, Train Loss: 0.1065, Val Loss: 0.0060, Test Loss: 0.0060\n","Epoch: 011, Train Loss: 0.0722, Val Loss: 0.0068, Test Loss: 0.0068\n","Epoch: 012, Train Loss: 0.0855, Val Loss: 0.0068, Test Loss: 0.0068\n","Epoch: 013, Train Loss: 0.0497, Val Loss: 0.0061, Test Loss: 0.0061\n","Epoch: 014, Train Loss: 0.0590, Val Loss: 0.0053, Test Loss: 0.0053\n","Epoch: 015, Train Loss: 0.0841, Val Loss: 0.0048, Test Loss: 0.0048\n","Epoch: 016, Train Loss: 0.0682, Val Loss: 0.0046, Test Loss: 0.0046\n","Epoch: 017, Train Loss: 0.0633, Val Loss: 0.0045, Test Loss: 0.0045\n","Epoch: 018, Train Loss: 0.0650, Val Loss: 0.0045, Test Loss: 0.0045\n","Epoch: 019, Train Loss: 0.0984, Val Loss: 0.0046, Test Loss: 0.0046\n","Epoch: 020, Train Loss: 0.0689, Val Loss: 0.0046, Test Loss: 0.0046\n","Epoch: 021, Train Loss: 0.0636, Val Loss: 0.0046, Test Loss: 0.0046\n","Epoch: 022, Train Loss: 0.0735, Val Loss: 0.0045, Test Loss: 0.0045\n","Epoch: 023, Train Loss: 0.0621, Val Loss: 0.0044, Test Loss: 0.0044\n","Epoch: 024, Train Loss: 0.0527, Val Loss: 0.0044, Test Loss: 0.0044\n","Epoch: 025, Train Loss: 0.0878, Val Loss: 0.0044, Test Loss: 0.0044\n","Epoch: 026, Train Loss: 0.0767, Val Loss: 0.0045, Test Loss: 0.0045\n","Epoch: 027, Train Loss: 0.0696, Val Loss: 0.0046, Test Loss: 0.0046\n","Epoch: 028, Train Loss: 0.0949, Val Loss: 0.0046, Test Loss: 0.0046\n","Epoch: 029, Train Loss: 0.0765, Val Loss: 0.0048, Test Loss: 0.0048\n","Epoch: 030, Train Loss: 0.0701, Val Loss: 0.0050, Test Loss: 0.0050\n","Epoch: 031, Train Loss: 0.0515, Val Loss: 0.0051, Test Loss: 0.0051\n","Epoch: 032, Train Loss: 0.0535, Val Loss: 0.0051, Test Loss: 0.0051\n","Epoch: 033, Train Loss: 0.0733, Val Loss: 0.0049, Test Loss: 0.0049\n","Epoch: 034, Train Loss: 0.0539, Val Loss: 0.0047, Test Loss: 0.0047\n","Epoch: 035, Train Loss: 0.0669, Val Loss: 0.0048, Test Loss: 0.0048\n","Epoch: 036, Train Loss: 0.0768, Val Loss: 0.0051, Test Loss: 0.0051\n","Epoch: 037, Train Loss: 0.0618, Val Loss: 0.0052, Test Loss: 0.0052\n","Epoch: 038, Train Loss: 0.0522, Val Loss: 0.0052, Test Loss: 0.0052\n","Epoch: 039, Train Loss: 0.0893, Val Loss: 0.0051, Test Loss: 0.0051\n","Epoch: 040, Train Loss: 0.0799, Val Loss: 0.0052, Test Loss: 0.0052\n","Epoch: 041, Train Loss: 0.0590, Val Loss: 0.0052, Test Loss: 0.0052\n","Epoch: 042, Train Loss: 0.0522, Val Loss: 0.0052, Test Loss: 0.0052\n","Epoch: 043, Train Loss: 0.0810, Val Loss: 0.0052, Test Loss: 0.0052\n","Epoch: 044, Train Loss: 0.0696, Val Loss: 0.0055, Test Loss: 0.0055\n","Epoch: 045, Train Loss: 0.0617, Val Loss: 0.0055, Test Loss: 0.0055\n","Epoch: 046, Train Loss: 0.0673, Val Loss: 0.0054, Test Loss: 0.0054\n","Epoch: 047, Train Loss: 0.0649, Val Loss: 0.0051, Test Loss: 0.0051\n","Epoch: 048, Train Loss: 0.0581, Val Loss: 0.0049, Test Loss: 0.0049\n","Epoch: 049, Train Loss: 0.0599, Val Loss: 0.0048, Test Loss: 0.0048\n","Epoch: 050, Train Loss: 0.0439, Val Loss: 0.0048, Test Loss: 0.0048\n","Epoch: 051, Train Loss: 0.0724, Val Loss: 0.0047, Test Loss: 0.0047\n","Epoch: 052, Train Loss: 0.0849, Val Loss: 0.0047, Test Loss: 0.0047\n","Epoch: 053, Train Loss: 0.0663, Val Loss: 0.0047, Test Loss: 0.0047\n","Epoch: 054, Train Loss: 0.0659, Val Loss: 0.0047, Test Loss: 0.0047\n","Epoch: 055, Train Loss: 0.0510, Val Loss: 0.0045, Test Loss: 0.0045\n","Epoch: 056, Train Loss: 0.0509, Val Loss: 0.0043, Test Loss: 0.0043\n","Epoch: 057, Train Loss: 0.0605, Val Loss: 0.0041, Test Loss: 0.0041\n","Epoch: 058, Train Loss: 0.0429, Val Loss: 0.0040, Test Loss: 0.0040\n","Epoch: 059, Train Loss: 0.0510, Val Loss: 0.0040, Test Loss: 0.0040\n","Epoch: 060, Train Loss: 0.0690, Val Loss: 0.0040, Test Loss: 0.0040\n","Epoch: 061, Train Loss: 0.0506, Val Loss: 0.0040, Test Loss: 0.0040\n","Epoch: 062, Train Loss: 0.0624, Val Loss: 0.0039, Test Loss: 0.0039\n","Epoch: 063, Train Loss: 0.0606, Val Loss: 0.0037, Test Loss: 0.0037\n","Epoch: 064, Train Loss: 0.0524, Val Loss: 0.0036, Test Loss: 0.0036\n","Epoch: 065, Train Loss: 0.0595, Val Loss: 0.0037, Test Loss: 0.0037\n","Epoch: 066, Train Loss: 0.0654, Val Loss: 0.0039, Test Loss: 0.0039\n","Epoch: 067, Train Loss: 0.0642, Val Loss: 0.0040, Test Loss: 0.0040\n","Epoch: 068, Train Loss: 0.0423, Val Loss: 0.0043, Test Loss: 0.0043\n","Epoch: 069, Train Loss: 0.0335, Val Loss: 0.0045, Test Loss: 0.0045\n","Epoch: 070, Train Loss: 0.0679, Val Loss: 0.0045, Test Loss: 0.0045\n","Epoch: 071, Train Loss: 0.0531, Val Loss: 0.0042, Test Loss: 0.0042\n","Epoch: 072, Train Loss: 0.0570, Val Loss: 0.0038, Test Loss: 0.0038\n","Epoch: 073, Train Loss: 0.0523, Val Loss: 0.0036, Test Loss: 0.0036\n","Epoch: 074, Train Loss: 0.0438, Val Loss: 0.0036, Test Loss: 0.0036\n","Epoch: 075, Train Loss: 0.0491, Val Loss: 0.0037, Test Loss: 0.0037\n","Epoch: 076, Train Loss: 0.0502, Val Loss: 0.0038, Test Loss: 0.0038\n","Epoch: 077, Train Loss: 0.0384, Val Loss: 0.0039, Test Loss: 0.0039\n","Epoch: 078, Train Loss: 0.0439, Val Loss: 0.0039, Test Loss: 0.0039\n","Epoch: 079, Train Loss: 0.0389, Val Loss: 0.0039, Test Loss: 0.0039\n","Epoch: 080, Train Loss: 0.0404, Val Loss: 0.0038, Test Loss: 0.0038\n","Epoch: 081, Train Loss: 0.0464, Val Loss: 0.0037, Test Loss: 0.0037\n","Epoch: 082, Train Loss: 0.0460, Val Loss: 0.0035, Test Loss: 0.0035\n","Epoch: 083, Train Loss: 0.0513, Val Loss: 0.0034, Test Loss: 0.0034\n","Epoch: 084, Train Loss: 0.0420, Val Loss: 0.0034, Test Loss: 0.0034\n","Epoch: 085, Train Loss: 0.0463, Val Loss: 0.0033, Test Loss: 0.0033\n","Epoch: 086, Train Loss: 0.0358, Val Loss: 0.0032, Test Loss: 0.0032\n","Epoch: 087, Train Loss: 0.0453, Val Loss: 0.0032, Test Loss: 0.0032\n","Epoch: 088, Train Loss: 0.0424, Val Loss: 0.0031, Test Loss: 0.0031\n","Epoch: 089, Train Loss: 0.0363, Val Loss: 0.0031, Test Loss: 0.0031\n","Epoch: 090, Train Loss: 0.0394, Val Loss: 0.0032, Test Loss: 0.0032\n","Epoch: 091, Train Loss: 0.0375, Val Loss: 0.0032, Test Loss: 0.0032\n","Epoch: 092, Train Loss: 0.0589, Val Loss: 0.0032, Test Loss: 0.0032\n","Epoch: 093, Train Loss: 0.0438, Val Loss: 0.0032, Test Loss: 0.0032\n","Epoch: 094, Train Loss: 0.0323, Val Loss: 0.0032, Test Loss: 0.0032\n","Epoch: 095, Train Loss: 0.0427, Val Loss: 0.0031, Test Loss: 0.0031\n","Epoch: 096, Train Loss: 0.0541, Val Loss: 0.0031, Test Loss: 0.0031\n","Epoch: 097, Train Loss: 0.0494, Val Loss: 0.0030, Test Loss: 0.0030\n","Epoch: 098, Train Loss: 0.0460, Val Loss: 0.0030, Test Loss: 0.0030\n","Epoch: 099, Train Loss: 0.0419, Val Loss: 0.0029, Test Loss: 0.0029\n","Epoch: 100, Train Loss: 0.0408, Val Loss: 0.0029, Test Loss: 0.0029\n","Epoch: 101, Train Loss: 0.0471, Val Loss: 0.0029, Test Loss: 0.0029\n","Epoch: 102, Train Loss: 0.0406, Val Loss: 0.0028, Test Loss: 0.0028\n","Epoch: 103, Train Loss: 0.0484, Val Loss: 0.0027, Test Loss: 0.0027\n","Epoch: 104, Train Loss: 0.0420, Val Loss: 0.0027, Test Loss: 0.0027\n","Epoch: 105, Train Loss: 0.0288, Val Loss: 0.0027, Test Loss: 0.0027\n","Epoch: 106, Train Loss: 0.0473, Val Loss: 0.0026, Test Loss: 0.0026\n","Epoch: 107, Train Loss: 0.0359, Val Loss: 0.0026, Test Loss: 0.0026\n","Epoch: 108, Train Loss: 0.0423, Val Loss: 0.0026, Test Loss: 0.0026\n","Epoch: 109, Train Loss: 0.0430, Val Loss: 0.0026, Test Loss: 0.0026\n","Epoch: 110, Train Loss: 0.0320, Val Loss: 0.0026, Test Loss: 0.0026\n","Epoch: 111, Train Loss: 0.0437, Val Loss: 0.0026, Test Loss: 0.0026\n","Epoch: 112, Train Loss: 0.0374, Val Loss: 0.0026, Test Loss: 0.0026\n","Epoch: 113, Train Loss: 0.0480, Val Loss: 0.0027, Test Loss: 0.0027\n","Epoch: 114, Train Loss: 0.0276, Val Loss: 0.0027, Test Loss: 0.0027\n","Epoch: 115, Train Loss: 0.0472, Val Loss: 0.0027, Test Loss: 0.0027\n","Epoch: 116, Train Loss: 0.0319, Val Loss: 0.0028, Test Loss: 0.0028\n","Epoch: 117, Train Loss: 0.0482, Val Loss: 0.0030, Test Loss: 0.0030\n","Epoch: 118, Train Loss: 0.0285, Val Loss: 0.0030, Test Loss: 0.0030\n","Epoch: 119, Train Loss: 0.0386, Val Loss: 0.0029, Test Loss: 0.0029\n","Epoch: 120, Train Loss: 0.0441, Val Loss: 0.0028, Test Loss: 0.0028\n","Epoch: 121, Train Loss: 0.0331, Val Loss: 0.0027, Test Loss: 0.0027\n","Epoch: 122, Train Loss: 0.0349, Val Loss: 0.0027, Test Loss: 0.0027\n","Epoch: 123, Train Loss: 0.0287, Val Loss: 0.0027, Test Loss: 0.0027\n","Epoch: 124, Train Loss: 0.0363, Val Loss: 0.0027, Test Loss: 0.0027\n","Epoch: 125, Train Loss: 0.0394, Val Loss: 0.0026, Test Loss: 0.0026\n","Epoch: 126, Train Loss: 0.0343, Val Loss: 0.0026, Test Loss: 0.0026\n","Epoch: 127, Train Loss: 0.0266, Val Loss: 0.0025, Test Loss: 0.0025\n","Epoch: 128, Train Loss: 0.0366, Val Loss: 0.0026, Test Loss: 0.0026\n","Epoch: 129, Train Loss: 0.0429, Val Loss: 0.0029, Test Loss: 0.0029\n","Epoch: 130, Train Loss: 0.0451, Val Loss: 0.0031, Test Loss: 0.0031\n","Epoch: 131, Train Loss: 0.0419, Val Loss: 0.0031, Test Loss: 0.0031\n","Epoch: 132, Train Loss: 0.0248, Val Loss: 0.0032, Test Loss: 0.0032\n","Epoch: 133, Train Loss: 0.0264, Val Loss: 0.0031, Test Loss: 0.0031\n","Epoch: 134, Train Loss: 0.0394, Val Loss: 0.0030, Test Loss: 0.0030\n","Epoch: 135, Train Loss: 0.0316, Val Loss: 0.0027, Test Loss: 0.0027\n","Epoch: 136, Train Loss: 0.0460, Val Loss: 0.0025, Test Loss: 0.0025\n","Epoch: 137, Train Loss: 0.0372, Val Loss: 0.0024, Test Loss: 0.0024\n","Epoch: 138, Train Loss: 0.0345, Val Loss: 0.0023, Test Loss: 0.0023\n","Epoch: 139, Train Loss: 0.0258, Val Loss: 0.0023, Test Loss: 0.0023\n","Epoch: 140, Train Loss: 0.0289, Val Loss: 0.0024, Test Loss: 0.0024\n","Epoch: 141, Train Loss: 0.0341, Val Loss: 0.0023, Test Loss: 0.0023\n","Epoch: 142, Train Loss: 0.0303, Val Loss: 0.0023, Test Loss: 0.0023\n","Epoch: 143, Train Loss: 0.0287, Val Loss: 0.0022, Test Loss: 0.0022\n","Epoch: 144, Train Loss: 0.0293, Val Loss: 0.0023, Test Loss: 0.0023\n","Epoch: 145, Train Loss: 0.0340, Val Loss: 0.0024, Test Loss: 0.0024\n","Epoch: 146, Train Loss: 0.0224, Val Loss: 0.0025, Test Loss: 0.0025\n","Epoch: 147, Train Loss: 0.0302, Val Loss: 0.0026, Test Loss: 0.0026\n","Epoch: 148, Train Loss: 0.0294, Val Loss: 0.0027, Test Loss: 0.0027\n","Epoch: 149, Train Loss: 0.0300, Val Loss: 0.0026, Test Loss: 0.0026\n","Epoch: 150, Train Loss: 0.0268, Val Loss: 0.0025, Test Loss: 0.0025\n","Epoch: 151, Train Loss: 0.0235, Val Loss: 0.0024, Test Loss: 0.0024\n","Epoch: 152, Train Loss: 0.0301, Val Loss: 0.0023, Test Loss: 0.0023\n","Epoch: 153, Train Loss: 0.0265, Val Loss: 0.0022, Test Loss: 0.0022\n","Epoch: 154, Train Loss: 0.0320, Val Loss: 0.0021, Test Loss: 0.0021\n","Epoch: 155, Train Loss: 0.0310, Val Loss: 0.0021, Test Loss: 0.0021\n","Epoch: 156, Train Loss: 0.0224, Val Loss: 0.0021, Test Loss: 0.0021\n","Epoch: 157, Train Loss: 0.0240, Val Loss: 0.0022, Test Loss: 0.0022\n","Epoch: 158, Train Loss: 0.0237, Val Loss: 0.0022, Test Loss: 0.0022\n","Epoch: 159, Train Loss: 0.0337, Val Loss: 0.0021, Test Loss: 0.0021\n","Epoch: 160, Train Loss: 0.0285, Val Loss: 0.0021, Test Loss: 0.0021\n","Epoch: 161, Train Loss: 0.0298, Val Loss: 0.0021, Test Loss: 0.0021\n","Epoch: 162, Train Loss: 0.0299, Val Loss: 0.0021, Test Loss: 0.0021\n","Epoch: 163, Train Loss: 0.0240, Val Loss: 0.0021, Test Loss: 0.0021\n","Epoch: 164, Train Loss: 0.0333, Val Loss: 0.0021, Test Loss: 0.0021\n","Epoch: 165, Train Loss: 0.0221, Val Loss: 0.0021, Test Loss: 0.0021\n","Epoch: 166, Train Loss: 0.0282, Val Loss: 0.0020, Test Loss: 0.0020\n","Epoch: 167, Train Loss: 0.0218, Val Loss: 0.0020, Test Loss: 0.0020\n","Epoch: 168, Train Loss: 0.0271, Val Loss: 0.0019, Test Loss: 0.0019\n","Epoch: 169, Train Loss: 0.0253, Val Loss: 0.0019, Test Loss: 0.0019\n","Epoch: 170, Train Loss: 0.0244, Val Loss: 0.0019, Test Loss: 0.0019\n","Epoch: 171, Train Loss: 0.0293, Val Loss: 0.0019, Test Loss: 0.0019\n","Epoch: 172, Train Loss: 0.0238, Val Loss: 0.0019, Test Loss: 0.0019\n","Epoch: 173, Train Loss: 0.0254, Val Loss: 0.0018, Test Loss: 0.0018\n","Epoch: 174, Train Loss: 0.0231, Val Loss: 0.0018, Test Loss: 0.0018\n","Epoch: 175, Train Loss: 0.0248, Val Loss: 0.0018, Test Loss: 0.0018\n","Epoch: 176, Train Loss: 0.0325, Val Loss: 0.0018, Test Loss: 0.0018\n","Epoch: 177, Train Loss: 0.0232, Val Loss: 0.0017, Test Loss: 0.0017\n","Epoch: 178, Train Loss: 0.0253, Val Loss: 0.0017, Test Loss: 0.0017\n","Epoch: 179, Train Loss: 0.0219, Val Loss: 0.0017, Test Loss: 0.0017\n","Epoch: 180, Train Loss: 0.0266, Val Loss: 0.0017, Test Loss: 0.0017\n","Epoch: 181, Train Loss: 0.0189, Val Loss: 0.0017, Test Loss: 0.0017\n","Epoch: 182, Train Loss: 0.0161, Val Loss: 0.0016, Test Loss: 0.0016\n","Epoch: 183, Train Loss: 0.0322, Val Loss: 0.0016, Test Loss: 0.0016\n","Epoch: 184, Train Loss: 0.0276, Val Loss: 0.0016, Test Loss: 0.0016\n","Epoch: 185, Train Loss: 0.0214, Val Loss: 0.0016, Test Loss: 0.0016\n","Epoch: 186, Train Loss: 0.0282, Val Loss: 0.0016, Test Loss: 0.0016\n","Epoch: 187, Train Loss: 0.0241, Val Loss: 0.0016, Test Loss: 0.0016\n","Epoch: 188, Train Loss: 0.0257, Val Loss: 0.0016, Test Loss: 0.0016\n","Epoch: 189, Train Loss: 0.0252, Val Loss: 0.0016, Test Loss: 0.0016\n","Epoch: 190, Train Loss: 0.0219, Val Loss: 0.0016, Test Loss: 0.0016\n","Epoch: 191, Train Loss: 0.0253, Val Loss: 0.0016, Test Loss: 0.0016\n","Epoch: 192, Train Loss: 0.0249, Val Loss: 0.0015, Test Loss: 0.0015\n","Epoch: 193, Train Loss: 0.0230, Val Loss: 0.0015, Test Loss: 0.0015\n","Epoch: 194, Train Loss: 0.0180, Val Loss: 0.0015, Test Loss: 0.0015\n","Epoch: 195, Train Loss: 0.0324, Val Loss: 0.0015, Test Loss: 0.0015\n","Epoch: 196, Train Loss: 0.0273, Val Loss: 0.0014, Test Loss: 0.0014\n","Epoch: 197, Train Loss: 0.0263, Val Loss: 0.0014, Test Loss: 0.0014\n","Epoch: 198, Train Loss: 0.0203, Val Loss: 0.0014, Test Loss: 0.0014\n","Epoch: 199, Train Loss: 0.0246, Val Loss: 0.0014, Test Loss: 0.0014\n","Epoch: 200, Train Loss: 0.0272, Val Loss: 0.0014, Test Loss: 0.0014\n","Epoch: 201, Train Loss: 0.0167, Val Loss: 0.0014, Test Loss: 0.0014\n","Epoch: 202, Train Loss: 0.0276, Val Loss: 0.0014, Test Loss: 0.0014\n","Epoch: 203, Train Loss: 0.0310, Val Loss: 0.0014, Test Loss: 0.0014\n","Epoch: 204, Train Loss: 0.0237, Val Loss: 0.0014, Test Loss: 0.0014\n","Epoch: 205, Train Loss: 0.0225, Val Loss: 0.0014, Test Loss: 0.0014\n","Epoch: 206, Train Loss: 0.0215, Val Loss: 0.0014, Test Loss: 0.0014\n","Epoch: 207, Train Loss: 0.0214, Val Loss: 0.0014, Test Loss: 0.0014\n","Epoch: 208, Train Loss: 0.0289, Val Loss: 0.0014, Test Loss: 0.0014\n","Epoch: 209, Train Loss: 0.0195, Val Loss: 0.0014, Test Loss: 0.0014\n","Epoch: 210, Train Loss: 0.0245, Val Loss: 0.0013, Test Loss: 0.0013\n","Epoch: 211, Train Loss: 0.0272, Val Loss: 0.0013, Test Loss: 0.0013\n","Epoch: 212, Train Loss: 0.0253, Val Loss: 0.0014, Test Loss: 0.0014\n","Epoch: 213, Train Loss: 0.0184, Val Loss: 0.0014, Test Loss: 0.0014\n","Epoch: 214, Train Loss: 0.0257, Val Loss: 0.0014, Test Loss: 0.0014\n","Epoch: 215, Train Loss: 0.0237, Val Loss: 0.0013, Test Loss: 0.0013\n","Epoch: 216, Train Loss: 0.0197, Val Loss: 0.0013, Test Loss: 0.0013\n","Epoch: 217, Train Loss: 0.0169, Val Loss: 0.0013, Test Loss: 0.0013\n","Epoch: 218, Train Loss: 0.0179, Val Loss: 0.0013, Test Loss: 0.0013\n","Epoch: 219, Train Loss: 0.0201, Val Loss: 0.0012, Test Loss: 0.0012\n","Epoch: 220, Train Loss: 0.0212, Val Loss: 0.0012, Test Loss: 0.0012\n","Epoch: 221, Train Loss: 0.0173, Val Loss: 0.0012, Test Loss: 0.0012\n","Epoch: 222, Train Loss: 0.0157, Val Loss: 0.0012, Test Loss: 0.0012\n","Epoch: 223, Train Loss: 0.0284, Val Loss: 0.0012, Test Loss: 0.0012\n","Epoch: 224, Train Loss: 0.0231, Val Loss: 0.0012, Test Loss: 0.0012\n","Epoch: 225, Train Loss: 0.0167, Val Loss: 0.0012, Test Loss: 0.0012\n","Epoch: 226, Train Loss: 0.0200, Val Loss: 0.0012, Test Loss: 0.0012\n","Epoch: 227, Train Loss: 0.0256, Val Loss: 0.0012, Test Loss: 0.0012\n","Epoch: 228, Train Loss: 0.0175, Val Loss: 0.0012, Test Loss: 0.0012\n","Epoch: 229, Train Loss: 0.0130, Val Loss: 0.0012, Test Loss: 0.0012\n","Epoch: 230, Train Loss: 0.0202, Val Loss: 0.0013, Test Loss: 0.0013\n","Epoch: 231, Train Loss: 0.0139, Val Loss: 0.0013, Test Loss: 0.0013\n","Epoch: 232, Train Loss: 0.0207, Val Loss: 0.0013, Test Loss: 0.0013\n","Epoch: 233, Train Loss: 0.0199, Val Loss: 0.0012, Test Loss: 0.0012\n","Epoch: 234, Train Loss: 0.0172, Val Loss: 0.0012, Test Loss: 0.0012\n","Epoch: 235, Train Loss: 0.0135, Val Loss: 0.0012, Test Loss: 0.0012\n","Epoch: 236, Train Loss: 0.0187, Val Loss: 0.0011, Test Loss: 0.0011\n","Epoch: 237, Train Loss: 0.0178, Val Loss: 0.0011, Test Loss: 0.0011\n","Epoch: 238, Train Loss: 0.0214, Val Loss: 0.0011, Test Loss: 0.0011\n","Epoch: 239, Train Loss: 0.0225, Val Loss: 0.0011, Test Loss: 0.0011\n","Epoch: 240, Train Loss: 0.0155, Val Loss: 0.0011, Test Loss: 0.0011\n","Epoch: 241, Train Loss: 0.0197, Val Loss: 0.0010, Test Loss: 0.0010\n","Epoch: 242, Train Loss: 0.0192, Val Loss: 0.0010, Test Loss: 0.0010\n","Epoch: 243, Train Loss: 0.0127, Val Loss: 0.0010, Test Loss: 0.0010\n","Epoch: 244, Train Loss: 0.0163, Val Loss: 0.0010, Test Loss: 0.0010\n","Epoch: 245, Train Loss: 0.0160, Val Loss: 0.0010, Test Loss: 0.0010\n","Epoch: 246, Train Loss: 0.0189, Val Loss: 0.0010, Test Loss: 0.0010\n","Epoch: 247, Train Loss: 0.0197, Val Loss: 0.0010, Test Loss: 0.0010\n","Epoch: 248, Train Loss: 0.0144, Val Loss: 0.0010, Test Loss: 0.0010\n","Epoch: 249, Train Loss: 0.0132, Val Loss: 0.0010, Test Loss: 0.0010\n","Epoch: 250, Train Loss: 0.0147, Val Loss: 0.0010, Test Loss: 0.0010\n","Epoch: 251, Train Loss: 0.0225, Val Loss: 0.0010, Test Loss: 0.0010\n","Epoch: 252, Train Loss: 0.0188, Val Loss: 0.0010, Test Loss: 0.0010\n","Epoch: 253, Train Loss: 0.0236, Val Loss: 0.0010, Test Loss: 0.0010\n","Epoch: 254, Train Loss: 0.0182, Val Loss: 0.0009, Test Loss: 0.0009\n","Epoch: 255, Train Loss: 0.0149, Val Loss: 0.0009, Test Loss: 0.0009\n","Epoch: 256, Train Loss: 0.0122, Val Loss: 0.0009, Test Loss: 0.0009\n","Epoch: 257, Train Loss: 0.0121, Val Loss: 0.0009, Test Loss: 0.0009\n","Epoch: 258, Train Loss: 0.0166, Val Loss: 0.0009, Test Loss: 0.0009\n","Epoch: 259, Train Loss: 0.0193, Val Loss: 0.0009, Test Loss: 0.0009\n","Epoch: 260, Train Loss: 0.0137, Val Loss: 0.0009, Test Loss: 0.0009\n","Epoch: 261, Train Loss: 0.0148, Val Loss: 0.0009, Test Loss: 0.0009\n","Epoch: 262, Train Loss: 0.0112, Val Loss: 0.0009, Test Loss: 0.0009\n","Epoch: 263, Train Loss: 0.0165, Val Loss: 0.0009, Test Loss: 0.0009\n","Epoch: 264, Train Loss: 0.0151, Val Loss: 0.0009, Test Loss: 0.0009\n","Epoch: 265, Train Loss: 0.0230, Val Loss: 0.0009, Test Loss: 0.0009\n","Epoch: 266, Train Loss: 0.0132, Val Loss: 0.0009, Test Loss: 0.0009\n","Epoch: 267, Train Loss: 0.0098, Val Loss: 0.0009, Test Loss: 0.0009\n","Epoch: 268, Train Loss: 0.0167, Val Loss: 0.0009, Test Loss: 0.0009\n","Epoch: 269, Train Loss: 0.0137, Val Loss: 0.0009, Test Loss: 0.0009\n","Epoch: 270, Train Loss: 0.0175, Val Loss: 0.0009, Test Loss: 0.0009\n","Epoch: 271, Train Loss: 0.0143, Val Loss: 0.0008, Test Loss: 0.0008\n","Epoch: 272, Train Loss: 0.0135, Val Loss: 0.0008, Test Loss: 0.0008\n","Epoch: 273, Train Loss: 0.0138, Val Loss: 0.0008, Test Loss: 0.0008\n","Epoch: 274, Train Loss: 0.0148, Val Loss: 0.0009, Test Loss: 0.0009\n","Epoch: 275, Train Loss: 0.0124, Val Loss: 0.0008, Test Loss: 0.0008\n","Epoch: 276, Train Loss: 0.0130, Val Loss: 0.0008, Test Loss: 0.0008\n","Epoch: 277, Train Loss: 0.0125, Val Loss: 0.0008, Test Loss: 0.0008\n","Epoch: 278, Train Loss: 0.0175, Val Loss: 0.0008, Test Loss: 0.0008\n","Epoch: 279, Train Loss: 0.0145, Val Loss: 0.0008, Test Loss: 0.0008\n","Epoch: 280, Train Loss: 0.0114, Val Loss: 0.0008, Test Loss: 0.0008\n","Epoch: 281, Train Loss: 0.0144, Val Loss: 0.0007, Test Loss: 0.0007\n","Epoch: 282, Train Loss: 0.0129, Val Loss: 0.0007, Test Loss: 0.0007\n","Epoch: 283, Train Loss: 0.0130, Val Loss: 0.0007, Test Loss: 0.0007\n","Epoch: 284, Train Loss: 0.0101, Val Loss: 0.0007, Test Loss: 0.0007\n","Epoch: 285, Train Loss: 0.0123, Val Loss: 0.0007, Test Loss: 0.0007\n","Epoch: 286, Train Loss: 0.0084, Val Loss: 0.0007, Test Loss: 0.0007\n","Epoch: 287, Train Loss: 0.0084, Val Loss: 0.0007, Test Loss: 0.0007\n","Epoch: 288, Train Loss: 0.0105, Val Loss: 0.0007, Test Loss: 0.0007\n","Epoch: 289, Train Loss: 0.0147, Val Loss: 0.0007, Test Loss: 0.0007\n","Epoch: 290, Train Loss: 0.0139, Val Loss: 0.0007, Test Loss: 0.0007\n","Epoch: 291, Train Loss: 0.0109, Val Loss: 0.0007, Test Loss: 0.0007\n","Epoch: 292, Train Loss: 0.0151, Val Loss: 0.0007, Test Loss: 0.0007\n","Epoch: 293, Train Loss: 0.0177, Val Loss: 0.0007, Test Loss: 0.0007\n","Epoch: 294, Train Loss: 0.0138, Val Loss: 0.0007, Test Loss: 0.0007\n","Epoch: 295, Train Loss: 0.0133, Val Loss: 0.0007, Test Loss: 0.0007\n","Epoch: 296, Train Loss: 0.0113, Val Loss: 0.0007, Test Loss: 0.0007\n","Epoch: 297, Train Loss: 0.0091, Val Loss: 0.0007, Test Loss: 0.0007\n","Epoch: 298, Train Loss: 0.0130, Val Loss: 0.0006, Test Loss: 0.0006\n","Epoch: 299, Train Loss: 0.0100, Val Loss: 0.0006, Test Loss: 0.0006\n","Epoch: 300, Train Loss: 0.0107, Val Loss: 0.0006, Test Loss: 0.0006\n"]}],"source":["for epoch in range(1, num_epochs + 1):\n","    train_loss = train(train_loader)\n","    val_loss = test(val_loader)\n","    test_loss = test(test_loader)\n","    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Test Loss: {test_loss:.4f}')"]},{"cell_type":"markdown","metadata":{},"source":["## Comparazione"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T07:13:02.005884Z","iopub.status.busy":"2024-09-10T07:13:02.005486Z","iopub.status.idle":"2024-09-10T07:13:02.236593Z","shell.execute_reply":"2024-09-10T07:13:02.235293Z","shell.execute_reply.started":"2024-09-10T07:13:02.005847Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Nodi classificati per betweenness centrality predetta:\n","Nodo 14: Betweenness = 0.0029\n","Nodo 26: Betweenness = 0.0027\n","Nodo 29: Betweenness = 0.0020\n","Nodo 18: Betweenness = 0.0017\n","Nodo 9: Betweenness = 0.0012\n","Nodo 33: Betweenness = 0.0010\n","Nodo 2: Betweenness = 0.0006\n","Nodo 23: Betweenness = 0.0003\n","Nodo 28: Betweenness = 0.0000\n","Nodo 32: Betweenness = 0.0000\n","Nodo 15: Betweenness = -0.0003\n","Nodo 27: Betweenness = -0.0003\n","Nodo 30: Betweenness = -0.0003\n","Nodo 19: Betweenness = -0.0004\n","Nodo 8: Betweenness = -0.0004\n","Nodo 7: Betweenness = -0.0005\n","Nodo 13: Betweenness = -0.0007\n","Nodo 22: Betweenness = -0.0007\n","Nodo 3: Betweenness = -0.0007\n","Nodo 1: Betweenness = -0.0011\n","Nodo 12: Betweenness = -0.0014\n","Nodo 16: Betweenness = -0.0017\n","Nodo 5: Betweenness = -0.0019\n","Nodo 21: Betweenness = -0.0020\n","Nodo 31: Betweenness = -0.0021\n","Nodo 6: Betweenness = -0.0027\n","Nodo 0: Betweenness = -0.0032\n","Nodo 25: Betweenness = -0.0033\n","Nodo 4: Betweenness = -0.0034\n","Nodo 24: Betweenness = -0.0035\n","Nodo 17: Betweenness = -0.0035\n","Nodo 10: Betweenness = -0.0038\n","Nodo 20: Betweenness = -0.0066\n","Nodo 11: Betweenness = -0.0098\n"]}],"source":["from torch_geometric.datasets import KarateClub\n","\n","# Carica il dataset Karate Club\n","dataset_k = KarateClub()\n","data_k = dataset_k[0]\n","\n","# Aggiungi padding alle feature dei nodi per uniformare il numero di feature\n","data_k = add_padding_to_features(data_k, target_num_features)\n","\n","# Metti il modello in modalità di valutazione\n","model.eval()\n","\n","# Calcola la betweenness centrality usando il modello\n","with torch.no_grad():\n","    data_k = data_k.to(device)\n","    predicted_betweenness = model(data_k).squeeze()\n","\n","# Ordina i nodi per betweenness centrality predetta\n","sorted_indices = torch.argsort(predicted_betweenness, descending=True)\n","\n","# Stampa i nodi classificati per betweenness centrality\n","print(\"Nodi classificati per betweenness centrality predetta:\")\n","for idx in sorted_indices:\n","    print(f\"Nodo {idx.item()}: Betweenness = {predicted_betweenness[idx].item():.4f}\")"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.status.busy":"2024-09-10T07:06:38.887656Z","iopub.status.idle":"2024-09-10T07:06:38.888033Z","shell.execute_reply":"2024-09-10T07:06:38.887874Z","shell.execute_reply.started":"2024-09-10T07:06:38.887856Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Confronto tra betweenness centrality predetta ed esatta:\n","Nodo 14 (Predetto): Betweenness = 0.0029 | Nodo 0 (Esatto): Betweenness = 0.4376\n","Nodo 26 (Predetto): Betweenness = 0.0027 | Nodo 33 (Esatto): Betweenness = 0.3041\n","Nodo 29 (Predetto): Betweenness = 0.0020 | Nodo 32 (Esatto): Betweenness = 0.1452\n","Nodo 18 (Predetto): Betweenness = 0.0017 | Nodo 2 (Esatto): Betweenness = 0.1437\n","Nodo 9 (Predetto): Betweenness = 0.0012 | Nodo 31 (Esatto): Betweenness = 0.1383\n","Nodo 33 (Predetto): Betweenness = 0.0010 | Nodo 8 (Esatto): Betweenness = 0.0559\n","Nodo 2 (Predetto): Betweenness = 0.0006 | Nodo 1 (Esatto): Betweenness = 0.0539\n","Nodo 23 (Predetto): Betweenness = 0.0003 | Nodo 13 (Esatto): Betweenness = 0.0459\n","Nodo 28 (Predetto): Betweenness = 0.0000 | Nodo 19 (Esatto): Betweenness = 0.0325\n","Nodo 32 (Predetto): Betweenness = 0.0000 | Nodo 6 (Esatto): Betweenness = 0.0300\n","Nodo 15 (Predetto): Betweenness = -0.0003 | Nodo 5 (Esatto): Betweenness = 0.0300\n","Nodo 27 (Predetto): Betweenness = -0.0003 | Nodo 27 (Esatto): Betweenness = 0.0223\n","Nodo 30 (Predetto): Betweenness = -0.0003 | Nodo 23 (Esatto): Betweenness = 0.0176\n","Nodo 19 (Predetto): Betweenness = -0.0004 | Nodo 30 (Esatto): Betweenness = 0.0144\n","Nodo 8 (Predetto): Betweenness = -0.0004 | Nodo 3 (Esatto): Betweenness = 0.0119\n","Nodo 7 (Predetto): Betweenness = -0.0005 | Nodo 25 (Esatto): Betweenness = 0.0038\n","Nodo 13 (Predetto): Betweenness = -0.0007 | Nodo 29 (Esatto): Betweenness = 0.0029\n","Nodo 22 (Predetto): Betweenness = -0.0007 | Nodo 24 (Esatto): Betweenness = 0.0022\n","Nodo 3 (Predetto): Betweenness = -0.0007 | Nodo 28 (Esatto): Betweenness = 0.0018\n","Nodo 1 (Predetto): Betweenness = -0.0011 | Nodo 9 (Esatto): Betweenness = 0.0008\n","Nodo 12 (Predetto): Betweenness = -0.0014 | Nodo 10 (Esatto): Betweenness = 0.0006\n","Nodo 16 (Predetto): Betweenness = -0.0017 | Nodo 4 (Esatto): Betweenness = 0.0006\n","Nodo 5 (Predetto): Betweenness = -0.0019 | Nodo 22 (Esatto): Betweenness = 0.0000\n","Nodo 21 (Predetto): Betweenness = -0.0020 | Nodo 21 (Esatto): Betweenness = 0.0000\n","Nodo 31 (Predetto): Betweenness = -0.0021 | Nodo 20 (Esatto): Betweenness = 0.0000\n","Nodo 6 (Predetto): Betweenness = -0.0027 | Nodo 18 (Esatto): Betweenness = 0.0000\n","Nodo 0 (Predetto): Betweenness = -0.0032 | Nodo 26 (Esatto): Betweenness = 0.0000\n","Nodo 25 (Predetto): Betweenness = -0.0033 | Nodo 16 (Esatto): Betweenness = 0.0000\n","Nodo 4 (Predetto): Betweenness = -0.0034 | Nodo 15 (Esatto): Betweenness = 0.0000\n","Nodo 24 (Predetto): Betweenness = -0.0035 | Nodo 14 (Esatto): Betweenness = 0.0000\n","Nodo 17 (Predetto): Betweenness = -0.0035 | Nodo 12 (Esatto): Betweenness = 0.0000\n","Nodo 10 (Predetto): Betweenness = -0.0038 | Nodo 11 (Esatto): Betweenness = 0.0000\n","Nodo 20 (Predetto): Betweenness = -0.0066 | Nodo 7 (Esatto): Betweenness = 0.0000\n","Nodo 11 (Predetto): Betweenness = -0.0098 | Nodo 17 (Esatto): Betweenness = 0.0000\n"]}],"source":["# Funzione per calcolare la betweenness centrality esatta\n","def calculate_exact_betweenness(data):\n","    G = to_networkx(data, to_undirected=True)\n","    betweenness = nx.betweenness_centrality(G)\n","    return torch.tensor([betweenness[i] for i in range(data.num_nodes)], dtype=torch.float)\n","\n","# Calcola la betweenness centrality esatta\n","exact_betweenness = calculate_exact_betweenness(data_k)\n","\n","# Ordina i nodi per betweenness centrality predetta\n","sorted_indices_predicted = torch.argsort(predicted_betweenness, descending=True)\n","\n","# Ordina i nodi per betweenness centrality esatta\n","sorted_indices_exact = torch.argsort(exact_betweenness, descending=True)\n","\n","# Confronta i nodi classificati per betweenness centrality predetta ed esatta\n","print(\"Confronto tra betweenness centrality predetta ed esatta:\")\n","for idx in range(data_k.num_nodes):\n","    node_pred = sorted_indices_predicted[idx].item()\n","    node_exact = sorted_indices_exact[idx].item()\n","    print(f\"Nodo {node_pred} (Predetto): Betweenness = {predicted_betweenness[node_pred]:.4f} | Nodo {node_exact} (Esatto): Betweenness = {exact_betweenness[node_exact]:.4f}\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5651278,"sourceId":9327792,"sourceType":"datasetVersion"},{"datasetId":5671176,"sourceId":9354976,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":4}
